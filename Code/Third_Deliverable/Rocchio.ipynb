{
 "cells": [
  {
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch_dsl import Search\n",
    "from elasticsearch_dsl.query import Q\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from elasticsearch.client import CatClient"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(tw):\n",
    "    \"\"\"\n",
    "    Normalizes the weights in t so that they form a unit-length vector\n",
    "    It is assumed that not all weights are 0\n",
    "    :param tw:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    mod = np.sqrt(np.sum([x**2 for x in tw.values()]))\n",
    "    return {t: tw[t]/mod for t in tw.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_term_vector(client, index, id):\n",
    "    \"\"\"\n",
    "    Returns the term vector of a document and its statistics a two sorted list of pairs (word, count)\n",
    "    The first one is the frequency of the term in the document, the second one is the number of documents\n",
    "    that contain the term\n",
    "\n",
    "    :param client:\n",
    "    :param index:\n",
    "    :param id:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    termvector = client.termvectors(index=index, id=id, fields=['text'],\n",
    "                                    positions=False, term_statistics=True)\n",
    "\n",
    "    file_td = {}\n",
    "    file_df = {}\n",
    "\n",
    "    if 'text' in termvector['term_vectors']:\n",
    "        for t in termvector['term_vectors']['text']['terms']:\n",
    "            file_td[t] = termvector['term_vectors']['text']['terms'][t]['term_freq']\n",
    "            file_df[t] = termvector['term_vectors']['text']['terms'][t]['doc_freq']\n",
    "    return sorted(file_td.items()), sorted(file_df.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_count(client, index):\n",
    "    \"\"\"\n",
    "    Returns the number of documents in an index\n",
    "\n",
    "    :param client:\n",
    "    :param index:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    return int(CatClient(client).count(index=[index], format='json')[0]['count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toTFIDF(client, index, file_id):\n",
    "    \"\"\"\n",
    "    Returns the term weights of a document\n",
    "\n",
    "    :param file:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the frequency of the term in the document, and the number of documents\n",
    "    # that contain the term\n",
    "    file_tv, file_df = document_term_vector(client, index, file_id)\n",
    "\n",
    "    max_freq = max([f for _, f in file_tv])\n",
    "\n",
    "    dcount = doc_count(client, index)\n",
    "\n",
    "    tfidfw = {}\n",
    "\n",
    "    for (t, w),(_, df) in zip(file_tv, file_df):\n",
    "        tf = w / max_freq\n",
    "        idf = np.log2(dcount/df)\n",
    "        tfidfw[t]= tf*idf\n",
    "\n",
    "    return normalize(tfidfw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(words_set, index, client, K):\n",
    "\n",
    "    s = Search(using=client, index=index)\n",
    "    #print(words_set.columns[0] + '^' + str(words_set[words_set.columns[0]].values[0]))\n",
    "    q = Q('query_string',query=words_set.columns[0] + '^' + str(words_set[words_set.columns[0]].values[0])) \n",
    "\n",
    "    for elem in words_set.columns[1:]:\n",
    "        #print(elem + '^' + str(words_set[elem].values[0]))\n",
    "        q &= Q('query_string',query=elem + '^' + str(words_set[elem].values[0]))\n",
    "\n",
    "    s = s.query(q)\n",
    "    response = s[0:K].execute()\n",
    "    results = pd.DataFrame(index=['Weight'])\n",
    "    for r in response:  # only returns a specific number of results\n",
    "        results = results.add(pd.DataFrame(toTFIDF(client, index, r.meta.id), index=['Weight']), fill_value=0)\n",
    "    \n",
    "    return results.sort_values(by ='Weight', axis=1, ascending=False).div(K)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "             nsa     trust       i'd    happy  government\n",
       "Weight  0.212683  0.180221  0.135717  0.12022    0.079946"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>nsa</th>\n      <th>trust</th>\n      <th>i'd</th>\n      <th>happy</th>\n      <th>government</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Weight</th>\n      <td>0.212683</td>\n      <td>0.180221</td>\n      <td>0.135717</td>\n      <td>0.12022</td>\n      <td>0.079946</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "'''Declaration of variables to be used later'''\n",
    "\n",
    "index = 'news'\n",
    "beta = 0.6\n",
    "alpha = 0.4\n",
    "initial_query = input().split(' ')\n",
    "words = pd.DataFrame(index=['Weight'], columns=[x.split('^')[0] for x in initial_query], data=[[1 if '^' not in el else el.split('^')[1] for el in initial_query]])\n",
    "k = 5\n",
    "client = Elasticsearch()\n",
    "R = 5\n",
    "nrounds = 20\n",
    "\n",
    "for _ in range(nrounds):\n",
    "    Res = search(words, index, client, k)*beta\n",
    "    words = words*alpha\n",
    "    words = words.add(Res, fill_value=0).sort_values(by ='Weight', axis=1, ascending=False)\n",
    "    words = words[words.columns[:R]]\n",
    "words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "PATH= C:\\CAIM\\Data\\Dirty\\20_newsgroups\\sci.crypt/0011219\n"
     ]
    }
   ],
   "source": [
    "s = Search(using=client, index=index)\n",
    "q = Q('query_string',query=words.columns[0] + '^' + str(words[words.columns[0]].values[0])) \n",
    "\n",
    "for elem in words.columns[1:]:\n",
    "    q &= Q('query_string',query=elem + '^' + str(words[elem].values[0]))\n",
    "\n",
    "s = s.query(q)\n",
    "response = s[0].execute()\n",
    "for r in response:  # only returns a specific number of results\n",
    "    print(f'PATH= {r.path}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "Python 3.8.3 64-bit (conda)",
   "display_name": "Python 3.8.3 64-bit (conda)",
   "metadata": {
    "interpreter": {
     "hash": "f0147c853d052fb7233589d4fd0b86cfbae0e542432001d5878e77688901c03a"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.8.3-final"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "66px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}