{
 "cells": [
  {
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch_dsl import Search\n",
    "from elasticsearch_dsl.query import Q\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from elasticsearch.client import CatClient"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(tw):\n",
    "    \"\"\"\n",
    "    Normalizes the weights in t so that they form a unit-length vector\n",
    "    It is assumed that not all weights are 0\n",
    "    :param tw:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    mod = np.sqrt(np.sum([x**2 for x in tw.values()]))\n",
    "    return {t: tw[t]/mod for t in tw.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_term_vector(client, index, id):\n",
    "    \"\"\"\n",
    "    Returns the term vector of a document and its statistics a two sorted list of pairs (word, count)\n",
    "    The first one is the frequency of the term in the document, the second one is the number of documents\n",
    "    that contain the term\n",
    "\n",
    "    :param client:\n",
    "    :param index:\n",
    "    :param id:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    termvector = client.termvectors(index=index, id=id, fields=['text'],\n",
    "                                    positions=False, term_statistics=True)\n",
    "\n",
    "    file_td = {}\n",
    "    file_df = {}\n",
    "\n",
    "    if 'text' in termvector['term_vectors']:\n",
    "        for t in termvector['term_vectors']['text']['terms']:\n",
    "            file_td[t] = termvector['term_vectors']['text']['terms'][t]['term_freq']\n",
    "            file_df[t] = termvector['term_vectors']['text']['terms'][t]['doc_freq']\n",
    "    return sorted(file_td.items()), sorted(file_df.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_count(client, index):\n",
    "    \"\"\"\n",
    "    Returns the number of documents in an index\n",
    "\n",
    "    :param client:\n",
    "    :param index:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    return int(CatClient(client).count(index=[index], format='json')[0]['count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toTFIDF(client, index, file_id):\n",
    "    \"\"\"\n",
    "    Returns the term weights of a document\n",
    "\n",
    "    :param file:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the frequency of the term in the document, and the number of documents\n",
    "    # that contain the term\n",
    "    file_tv, file_df = document_term_vector(client, index, file_id)\n",
    "\n",
    "    max_freq = max([f for _, f in file_tv])\n",
    "\n",
    "    dcount = doc_count(client, index)\n",
    "\n",
    "    tfidfw = {}\n",
    "\n",
    "    for (t, w),(_, df) in zip(file_tv, file_df):\n",
    "        tf = w / max_freq\n",
    "        idf = np.log2(dcount/df)\n",
    "        tfidfw[t]= tf*idf\n",
    "\n",
    "    return normalize(tfidfw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(words_set, index, client, K):\n",
    "\n",
    "    s = Search(using=client, index=index)\n",
    "    print(words_set.columns[0])\n",
    "    q = Q('query_string',query=words_set.columns[0]) \n",
    "\n",
    "    for elem in words_set.columns[1:]:\n",
    "        print(elem)\n",
    "        q &= Q('query_string',query=elem)\n",
    "\n",
    "    s = s.query(q)\n",
    "    response = s[0:K].execute()\n",
    "    results = pd.DataFrame(index=['Weight'])\n",
    "    for r in response:  # only returns a specific number of results\n",
    "        results = results.add(pd.DataFrame(toTFIDF(client, index, r.meta.id), index=['Weight']), fill_value=0)\n",
    "    \n",
    "    return results.sort_values(by ='Weight', axis=1, ascending=False).div(K)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''Declaration of variables to be used later'''\n",
    "\n",
    "index = 'news'\n",
    "beta = 0.6\n",
    "alpha = 0.4\n",
    "initial_query = input().split(' ')\n",
    "words = pd.DataFrame(index=['Weight'], columns=[x.split('^')[0] for x in initial_query], data=[[1 if '^' not in el else el.split('^')[1] for el in initial_query]])\n",
    "k = 5\n",
    "client = Elasticsearch()\n",
    "\n",
    "nrounds = 20\n",
    "\n",
    "for _ in range(nrounds):\n",
    "    R = search(words, index, client, k)*beta\n",
    "    words = words*alpha\n",
    "    words = words.add(R, fill_value=0).sort_values(by ='Weight', axis=1, ascending=False)\n",
    "    words = words[words.columns[:5]]\n",
    "    print(words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## 2 Rocchio's Rule\n",
    "\n",
    "\n",
    "For implementing the relevance we are going to use the Rocchio's rule. We are going to extend the query for a number of interations using the terms in the more relevant documents that are retrieved.\n",
    "\n",
    "As is described in the session documentation you will need to write a scripts that given a query, repeats a number ($nrounds$) of times:\n",
    "\n",
    "1. Obtain the $k$ more relevant documents\n",
    "2. Compute a new query using the current query and the terms of the $k$ documents\n",
    "\n",
    "The Rocchio's rule involves computing the folowing:\n",
    "\n",
    "$$Query' = \t\\alpha \\times Query + \\beta \\times \\frac{d_1 + d_2 + \\cdots + d_k}{k}$$\n",
    "\n",
    "So we have different parameters to play with:\n",
    "\n",
    "1. The number of rounds ($nrounds$)\n",
    "2. The number of relevand documents ($k$)\n",
    "3. The parameters of the Rocchio's rule ($\\alpha$ and $\\beta$)\n",
    "4. The numbeer of terms in the recomputed query ($R$)\n",
    "\n",
    "**Read the documentation** and pay attention specially to how you have to build the query that you pass to ElasticSearch to include thw weights computed by the Rocchio's rule.\n",
    "\n",
    "Think that some of the elements that you need for this part are functions that you programmed already as part of the past session assignment.\n",
    "\n",
    "**Pay attention** to the documentation that you have to deliver for this session.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "Python 3.8.3 64-bit (conda)",
   "display_name": "Python 3.8.3 64-bit (conda)",
   "metadata": {
    "interpreter": {
     "hash": "f0147c853d052fb7233589d4fd0b86cfbae0e542432001d5878e77688901c03a"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.8.3-final"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "66px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}