{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch_dsl import Search\n",
    "from elasticsearch_dsl.query import Q\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from elasticsearch.client import CatClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(tw):\n",
    "    \"\"\"\n",
    "    Normalizes the weights in t so that they form a unit-length vector\n",
    "    It is assumed that not all weights are 0\n",
    "    :param tw:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    mod = np.sqrt(np.sum([x**2 for x in tw.values()]))\n",
    "    return {t: tw[t]/mod for t in tw.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_term_vector(client, index, id):\n",
    "    \"\"\"\n",
    "    Returns the term vector of a document and its statistics a two sorted list of pairs (word, count)\n",
    "    The first one is the frequency of the term in the document, the second one is the number of documents\n",
    "    that contain the term\n",
    "\n",
    "    :param client:\n",
    "    :param index:\n",
    "    :param id:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    termvector = client.termvectors(index=index, id=id, fields=['text'],\n",
    "                                    positions=False, term_statistics=True)\n",
    "\n",
    "    file_td = {}\n",
    "    file_df = {}\n",
    "\n",
    "    if 'text' in termvector['term_vectors']:\n",
    "        for t in termvector['term_vectors']['text']['terms']:\n",
    "            file_td[t] = termvector['term_vectors']['text']['terms'][t]['term_freq']\n",
    "            file_df[t] = termvector['term_vectors']['text']['terms'][t]['doc_freq']\n",
    "    return sorted(file_td.items()), sorted(file_df.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_count(client, index):\n",
    "    \"\"\"\n",
    "    Returns the number of documents in an index\n",
    "\n",
    "    :param client:\n",
    "    :param index:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    return int(CatClient(client).count(index=[index], format='json')[0]['count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toTFIDF(client, index, file_id):\n",
    "    \"\"\"\n",
    "    Returns the term weights of a document\n",
    "\n",
    "    :param file:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the frequency of the term in the document, and the number of documents\n",
    "    # that contain the term\n",
    "    file_tv, file_df = document_term_vector(client, index, file_id)\n",
    "\n",
    "    max_freq = max([f for _, f in file_tv])\n",
    "\n",
    "    dcount = doc_count(client, index)\n",
    "\n",
    "    tfidfw = {}\n",
    "\n",
    "    for (t, w),(_, df) in zip(file_tv, file_df):\n",
    "        tf = w / max_freq\n",
    "        idf = np.log2(dcount/df)\n",
    "        tfidfw[t]= tf*idf\n",
    "\n",
    "    return tfidfw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def div_all_map(elem, value):\n",
    "    return elem/value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(words_set, index, client, K, R):\n",
    "\n",
    "    s = Search(using=client, index=index)\n",
    "    set_query_elements = [k+'^'+str(v) for (k,v) in zip(words_set.keys(),words_set.values())]\n",
    "    \n",
    "    q = Q('query_string',query=set_query_elements[0]) \n",
    "    for elem in set_query_elements[1:]:\n",
    "        q &= Q('query_string',query=elem)\n",
    "\n",
    "    s = s.query(q)\n",
    "    response = s[0:K].execute()\n",
    "    results = {}\n",
    "    for r in response:  # only returns a specific number of results\n",
    "        tfidf = toTFIDF(client, index, r.meta.id)\n",
    "        results = {t: tfidf.get(t, 0) + results.get(t, 0) for t in set(tfidf) | set(results)}\n",
    "    \n",
    "    results = {t: results[t]/K for t in results.keys()}\n",
    "    return normalize(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'friend': 0.9174827805204098,\n",
       " 'to': 0.2555584626014873,\n",
       " 'with': 0.3048199790717496}"
      ]
     },
     "metadata": {},
     "execution_count": 49
    }
   ],
   "source": [
    "'''Declaration of variables to be used later'''\n",
    "\n",
    "index = 'news'\n",
    "beta = 0.6\n",
    "alpha = 0.4\n",
    "initial_query = input().split(' ')\n",
    "nrounds = int(input())\n",
    "\n",
    "\n",
    "l1 = [x for x in initial_query if '^' in x]\n",
    "l2 = [x + '^1' for x in initial_query if '^' not in x]\n",
    "words = {k.split('^')[0]:int(k.split('^')[1]) for k in l1+l2}\n",
    "words = normalize(words)\n",
    "k = 60\n",
    "client = Elasticsearch()\n",
    "R = 3\n",
    "for _ in range(nrounds):\n",
    "    Res = search(words, index, client, k, R)\n",
    "    Res = {t:Res[t]*beta for t in Res.keys()}\n",
    "    words = {k:words[k]*alpha for k in words.keys()}\n",
    "    words = {t: words.get(t, 0) + Res.get(t, 0) for t in set(words) | set(Res)}\n",
    "    words = {key: value for key, value in words.items() if value in sorted(set(words.values()), reverse=True)[:R]}\n",
    "    words = normalize(words)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = Search(using=client, index=index)\n",
    "set_query_elements = [k+'^'+str(v) for (k,v) in zip(words.keys(),words.values())]\n",
    "q = Q('query_string',query=set_query_elements[0]) \n",
    "\n",
    "for elem in set_query_elements:\n",
    "    q &= Q('query_string',query=elem)\n",
    "\n",
    "s = s.query(q)\n",
    "response = s.execute()\n",
    "for r in response:  \n",
    "    print(f'PATH= {r.path}')"
   ]
  }
 ]
}